{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision as tv\n",
    "from torchvision import datasets as tvds\n",
    "from torchvision import transforms\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available else \"cpu\")\n",
    "# device = \"cpu\"\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Image dataset\n",
    "- Download the dataset\n",
    "- Add transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_transformer = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomAdjustSharpness(1.2),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomGrayscale(),\n",
    "        transforms.RandomRotation(degrees=180),\n",
    "        transforms.Resize(150),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((.5, .5, .5), (.5, .5, .5)),\n",
    "#         transforms.Lambda(lambda x: collate_fn(x))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tvds.CIFAR100(\"./cifar\", transform=im_transformer, download=True, train=True)\n",
    "test_dataset = tvds.CIFAR100(\"./cifar\", transform=im_transformer, download=True, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "There is a structure issue, need to fix it.\n",
    "\n",
    "Current structure\n",
    "[\n",
    "    {'x\": \"\", \"y\": \"\"}\n",
    "]\n",
    "\n",
    "Required Structure\n",
    "{\"x\": [\"\"], \"y\": [\"\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\r\n",
    "    x_, y_ = [], []\r\n",
    "    for (x, y) in batch:\r\n",
    "        x_.append(x)\r\n",
    "        y_.append(y)\r\n",
    "    return {\r\n",
    "        \"x\": torch.stack(x_).to(device),\r\n",
    "        \"y\": torch.from_numpy(np.array(y_, dtype=np.int64)).to(device)\r\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=2, collate_fn=collate_fn)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, shuffle=True, batch_size=1, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationModel(nn.Module):\r\n",
    "    \r\n",
    "    def __init__(self, out_dim, channel_first = False):\r\n",
    "        super(ClassificationModel, self).__init__()\r\n",
    "        self.input = nn.Conv2d(3, 256, (3, 3), padding=\"valid\")\r\n",
    "        self.conv1 = nn.Conv2d(256, 128, (3, 3))\r\n",
    "        self.maxp1 = nn.MaxPool2d((2, 2))\r\n",
    "#         self.conv2 = nn.Conv2d()\r\n",
    "#         self.conv1 = self.conv2DBlock(1024, 256, (5, 5))\r\n",
    "#         self.conv2 = self.conv2DBlock(1024, 1024, (3, 3))\r\n",
    "#         self.conv3 = self.conv2DBlock(1024, 512, (3, 3))\r\n",
    "        self.fc1 = nn.Linear(1024, 1024)\r\n",
    "        \r\n",
    "        self.out = nn.Linear(1024, out_dim)\r\n",
    "        \r\n",
    "#     def conv2DBlock(self, in_filter, out_filter, kernel_size):\r\n",
    "#         return nn.Sequential(\r\n",
    "#             nn.Conv2d(in_filter, in_filter, kernel_size, padding=\"same\"),\r\n",
    "#             nn.Conv2d(in_filter, in_filter, kernel_size, padding=\"same\"),\r\n",
    "#             nn.ReLU(),\r\n",
    "#             nn.MaxPool2d(kernel_size=(1,1)),\r\n",
    "#             nn.Conv2d(in_filter, out_filter, kernel_size, padding=\"valid\")\r\n",
    "#         )\r\n",
    "    \r\n",
    "    def forward(self, x):\r\n",
    "        x = self.input(x) # in: n x 3 x 150 x 150   out: n x 1024 x 148 x 148\r\n",
    "        x = self.conv1(x) # in: n x 1024 x 148 x 148  out: n x 512 x 146 x 146\r\n",
    "        x = F.relu(self.maxp1(x)) # in: n x 512 x 146 x 146  out: n x 512 x 73 x 73\r\n",
    "#         x = F.relu(self.conv2(x))\r\n",
    "#         x = F.relu(self.conv3(x))\r\n",
    "        x = x.view((-1, 128*73*73))\r\n",
    "        x = F.relu(self.fc1(x)) # in: n x 2728448 out: 1024\r\n",
    "#         x = F.relu(self.fc2(x))\r\n",
    "        return F.softmax(self.out(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassificationModel(\n",
       "  (input): Conv2d(3, 256, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n",
       "  (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (maxp1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (out): Linear(in_features=1024, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ClassificationModel(100)\n",
    "model = model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\r\n",
    "    def __init__(self, optimizer, criteria, epochs=10, scheduler=None):\r\n",
    "        \"\"\"\r\n",
    "        This class will train the model based on the \r\n",
    "        - optimizer: Optimizer algorithm (object)\r\n",
    "        - criteria: It is the loss function which will be used like CrossEntropyLoss, MSELoss etc.\r\n",
    "        \"\"\"\r\n",
    "        self.optimizer = optimizer\r\n",
    "        self.epochs = epochs\r\n",
    "        self.criteria = criteria\r\n",
    "        self.scheduler = scheduler\r\n",
    "        \r\n",
    "    \r\n",
    "    def train_one_step(self, x, y):\r\n",
    "        \"\"\"\r\n",
    "        Training on Single Step\r\n",
    "        - Predict the output\r\n",
    "        - Optimize the parameters\r\n",
    "        \"\"\"\r\n",
    "        self.optimizer.zero_grad() # Initialization of Gredients to 0\r\n",
    "        y_hat = self.model(x)\r\n",
    "        loss = self.criteria(y_hat, y)\r\n",
    "        loss.backward()\r\n",
    "        self.optimizer.step()\r\n",
    "        return loss.item()\r\n",
    "    \r\n",
    "    def train_one_epoch(self, data_loader):\r\n",
    "        \r\n",
    "        \"\"\"\r\n",
    "        This function will enable the epoch training and return the loss for the epoch\r\n",
    "        \"\"\"\r\n",
    "        self.model.train() # Setting model in Training Mode\r\n",
    "        total_loss = 0\r\n",
    "        for idx, data in tqdm(enumerate(data_loader), total=len(data_loader)):\r\n",
    "            loss = self.train_one_step(**data)\r\n",
    "            data.detach()\r\n",
    "            total_loss += loss\r\n",
    "        return total_loss / (idx + 1)\r\n",
    "    \r\n",
    "    def eval_one_epoch(self, data_loader):\r\n",
    "        \"\"\"\r\n",
    "        This function will enable the epoch training and return the loss for the epoch\r\n",
    "        \"\"\"\r\n",
    "        self.model.eval() # Setting model in Evaluation Mode\r\n",
    "        total_loss = 0\r\n",
    "        for idx, data in enumerate(data_loader):\r\n",
    "            x, y = data[\"x\"], data[\"y\"]\r\n",
    "            y_hat = self.model(x)\r\n",
    "            loss = self.criteria(y_hat, y)\r\n",
    "            total_loss += loss.item()\r\n",
    "        return total_loss / (idx + 1)\r\n",
    "    \r\n",
    "    def fit(self, model, train_loader, valid_loader=None, scheduler=None, **kwargs):\r\n",
    "        \"\"\"\r\n",
    "        This function will start the model training. \r\n",
    "        \"\"\"\r\n",
    "        self.model = model\r\n",
    "        valid_loss = None\r\n",
    "        for epoch in range(self.epochs):\r\n",
    "            loss = self.train_one_epoch(train_loader)\r\n",
    "            if valid_loader:\r\n",
    "                valid_loss = self.eval_one_epoch(valid_loader)\r\n",
    "            if hasattr(self, \"sechduler\") and self.sechduler != None:\r\n",
    "                self.scheduler.step()\r\n",
    "            tqdm.write(f\"Epoch: {epoch}, Training Loss: {loss}, Validation Loss: {valid_loss}\")\r\n",
    "        return self.model\r\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6da2f5b48f4049fc9fe5bcf11afbfd29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ritik\\miniconda3\\envs\\gpu\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "<ipython-input-8-824a742236f1>:34: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(self.out(x))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Function AddmmBackward returned an invalid gradient at index 1 - got [2, 1024] but expected shape compatible with [2, 682112]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-6aba36a71ca5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStepLR\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel_trainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriteria\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriteria\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mmodel_trainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-f6212d5fbcb4>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, train_loader, valid_loader, scheduler, **kwargs)\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[0mvalid_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_one_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m                 \u001b[0mvalid_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval_one_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-f6212d5fbcb4>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[1;34m(self, data_loader)\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_one_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-f6212d5fbcb4>\u001b[0m in \u001b[0;36mtrain_one_step\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0my_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcriteria\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\gpu\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 255\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\gpu\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Function AddmmBackward returned an invalid gradient at index 1 - got [2, 1024] but expected shape compatible with [2, 682112]"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.5, weight_decay=1e-5)\r\n",
    "criteria = nn.CrossEntropyLoss()\r\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5)\r\n",
    "model_trainer = Trainer(optimizer = optimizer, criteria = criteria, scheduler=scheduler, epochs=100)\r\n",
    "model_trainer.fit(model = model, train_loader=train_loader, valid_loader=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0ebe8b4b1754b029ca5f3828184c947296e41935aaaf6c3fc6186ced5a30495f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('gpu': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}